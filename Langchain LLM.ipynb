{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "050b5f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from secret_key import openapi_key\n",
    "# import os\n",
    "# os.environ['OPENAI_API_KEY'] = openapi_key\n",
    "\n",
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a02e557",
   "metadata": {},
   "outputs": [],
   "source": [
    "##there are many llm models also despite openai i.e Huggingface\n",
    "# openai has avariable temperature>>>>>temperature means how creative we want our model to be .....if temperature is set to 0...it means it is not at all creative if temperature=1..it is very creative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a27a025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\"Maharaja's Palace\" \n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0.9)\n",
    "name = llm.predict(\"I want to open a restaurant for Indian food. Suggest a fency name for this.\")\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adf2770",
   "metadata": {},
   "source": [
    "# Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b34a6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to open a restaurant for Italian food. Suggest a fency name for this.\n",
      "\n",
      "\n",
      "\"Bella Cucina\"\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables =['cuisine'],\n",
    "    template = \"I want to open a restaurant for {cuisine} food. Suggest a fency name for this.\"\n",
    ")\n",
    "p = prompt_template_name.format(cuisine=\"Italian\")\n",
    "print(p)\n",
    "\n",
    "print(llm.predict(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bba6aaf",
   "metadata": {},
   "source": [
    "# chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aca8be9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\"Casa del Sabor\"'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Here, we donot need to paas all the things we can just write code of one or two lines and it will give me the answer\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template_name)\n",
    "chain.run(\"Mexican\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61c5632e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0.9)\n",
    "\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables =['cuisine'],\n",
    "    template = \"I want to open a restaurant for {cuisine} food. Suggest a fency name for this.\"\n",
    ")\n",
    "\n",
    "name_chain =LLMChain(llm=llm, prompt=prompt_template_name)\n",
    "\n",
    "prompt_template_items = PromptTemplate(\n",
    "    input_variables = ['restaurant_name'],\n",
    "    template=\"\"\"Suggest some menu items for {restaurant_name}\"\"\"\n",
    ")\n",
    "\n",
    "food_items_chain = LLMChain(llm=llm, prompt=prompt_template_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06c1f40",
   "metadata": {},
   "source": [
    "# Simple Sequential Chain: Here output of 1st will be the input for second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8281fbab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Chicken Tikka Masala\n",
      "2. Butter Chicken\n",
      "3. Lamb Biryani\n",
      "4. Vegetable Samosas\n",
      "5. Garlic Naan\n",
      "6. Palak Paneer (spinach and cottage cheese curry)\n",
      "7. Tandoori Mixed Grill (assorted tandoori meats)\n",
      "8. Mango Lassi (yogurt-based drink)\n",
      "9. Malai Kofta (vegetarian meatballs in creamy tomato sauce)\n",
      "10. Kheer (Indian rice pudding)\n",
      "11. Chicken Vindaloo\n",
      "12. Rogan Josh (lamb in tomato and onion gravy)\n",
      "13. Aloo Gobi (potatoes and cauliflower curry)\n",
      "14. Tandoori Shrimp\n",
      "15. Naan Pizza (topped with tandoori chicken and vegetables)\n"
     ]
    }
   ],
   "source": [
    "###when we give restaurant name as input and it will give me the second component i.e food menu item then that will be the \n",
    "#sequential chain i.e the input of second step is the output for the first step\n",
    "\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "chain = SimpleSequentialChain(chains = [name_chain, food_items_chain])\n",
    "\n",
    "content = chain.run(\"Indian\")\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c224497e",
   "metadata": {},
   "source": [
    "# Sequential Chains: in this both restaurant name and food/menu items both can be get at the same time i.e there can be multiple outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6cdc3b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0.7)\n",
    "\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables =['cuisine'],\n",
    "    template = \"I want to open a restaurant for {cuisine} food. Suggest a fency name for this.\"\n",
    ")\n",
    "\n",
    "name_chain =LLMChain(llm=llm, prompt=prompt_template_name, output_key=\"restaurant_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "393b9c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0.7)\n",
    "\n",
    "prompt_template_items = PromptTemplate(\n",
    "    input_variables = ['restaurant_name'],\n",
    "    template=\"Suggest some menu items for {restaurant_name}.\"\n",
    ")\n",
    "\n",
    "food_items_chain =LLMChain(llm=llm, prompt=prompt_template_items, output_key=\"menu_items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e943b3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "\n",
    "chain = SequentialChain(\n",
    "    chains = [name_chain, food_items_chain],\n",
    "    input_variables = ['cuisine'],\n",
    "    output_variables = ['restaurant_name', \"menu_items\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7326c99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cuisine': 'Indian',\n",
       " 'restaurant_name': '\\n\\n\"Maharaja\\'s Palace\" ',\n",
       " 'menu_items': '\\n\\n1. Tandoori Chicken: Grilled chicken marinated in yogurt and spices, served with a side of mint chutney.\\n\\n2. Butter Chicken: Tender pieces of chicken simmered in a rich and creamy tomato-based sauce.\\n\\n3. Vegetable Biryani: Fragrant basmati rice cooked with mixed vegetables, nuts, and spices.\\n\\n4. Lamb Rogan Josh: Slow-cooked lamb curry with a blend of aromatic spices.\\n\\n5. Palak Paneer: Homemade cottage cheese cubes cooked in a creamy spinach gravy.\\n\\n6. Naan Bread: Traditional Indian flatbread baked in a clay oven.\\n\\n7. Samosas: Crispy pastry pockets filled with spiced potatoes and peas.\\n\\n8. Chicken Tikka Masala: Grilled chicken cooked in a creamy tomato and onion-based sauce.\\n\\n9. Dal Makhani: Slow-cooked black lentils and kidney beans in a rich and creamy sauce.\\n\\n10. Mango Lassi: A refreshing yogurt-based drink blended with fresh mangoes.\\n\\n11. Aloo Gobi: Potatoes and cauliflower cooked with onions, tomatoes, and spices.\\n\\n12. Gulab Jamun: A popular Indian dessert made with fried dough balls soaked in a sugary syrup.\\n\\n13. Chana Masala: Chickpeas'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain({\"cuisine\": \"Indian\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214c9da4",
   "metadata": {},
   "source": [
    "# Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "888413c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wikipedia\n",
      "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\nikit\\anaconda3\\lib\\site-packages (from wikipedia) (4.11.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in c:\\users\\nikit\\anaconda3\\lib\\site-packages (from wikipedia) (2.28.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nikit\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2023.7.22)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\nikit\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nikit\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\nikit\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\nikit\\anaconda3\\lib\\site-packages (from beautifulsoup4->wikipedia) (2.3.2.post1)\n",
      "Building wheels for collected packages: wikipedia\n",
      "  Building wheel for wikipedia (setup.py): started\n",
      "  Building wheel for wikipedia (setup.py): finished with status 'done'\n",
      "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11707 sha256=8bc3ad405b4fecddb26a06159080fb6077218ae8d2e094a43f57b5ddb923d695\n",
      "  Stored in directory: c:\\users\\nikit\\appdata\\local\\pip\\cache\\wheels\\b2\\7f\\26\\524faff9145e274da278dc97d63ab0bfde1f791ecf101a9c95\n",
      "Successfully built wikipedia\n",
      "Installing collected packages: wikipedia\n",
      "Successfully installed wikipedia-1.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7de2d779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I should use the calculator to add 5 to the GDP of US in 2022\n",
      "Action: Calculator\n",
      "Action Input: 22 trillion + 5\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 22000000000005\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: 22000000000005\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'22000000000005'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import AgentType, initialize_agent, load_tools\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "# The tools we'll give the Agent access to. Note that the 'llm-math' tool uses an LLM, so we need to pass that in.\n",
    "tools = load_tools([\"wikipedia\", \"llm-math\"], llm=llm)\n",
    "\n",
    "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "\n",
    "# Let's test it out!\n",
    "agent.run(\"What was the GDP of US in 2022 plus 5?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2880d2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure yoy have installed this package: pip install google-search-results\n",
    "# from secret_key import serpapi_key\n",
    "# os.environ['SERPAPI_API_KEY'] = serpapi_key\n",
    "\n",
    "os.environ['SERPAPI_API_KEY'] = \"a8f74238529545d281a520a19e9c63e742544114b8962d278deadb78ed03b855\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b4acbc79",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for SerpAPIWrapper\n__root__\n  Could not import serpapi python package. Please install it with `pip install google-search-results`. (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m llm \u001b[38;5;241m=\u001b[39m OpenAI(temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# The tools we'll give the Agent access to. Note that the 'llm-math' tool uses an LLM, so we need to pass that in.\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m tools \u001b[38;5;241m=\u001b[39m \u001b[43mload_tools\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mserpapi\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mllm-math\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\u001b[39;00m\n\u001b[0;32m     10\u001b[0m agent \u001b[38;5;241m=\u001b[39m initialize_agent(tools, llm, agent\u001b[38;5;241m=\u001b[39mAgentType\u001b[38;5;241m.\u001b[39mZERO_SHOT_REACT_DESCRIPTION, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\langchain\\agents\\load_tools.py:613\u001b[0m, in \u001b[0;36mload_tools\u001b[1;34m(tool_names, llm, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    611\u001b[0m     _get_tool_func, extra_keys \u001b[38;5;241m=\u001b[39m _EXTRA_OPTIONAL_TOOLS[name]\n\u001b[0;32m    612\u001b[0m     sub_kwargs \u001b[38;5;241m=\u001b[39m {k: kwargs[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m extra_keys \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m kwargs}\n\u001b[1;32m--> 613\u001b[0m     tool \u001b[38;5;241m=\u001b[39m _get_tool_func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msub_kwargs)\n\u001b[0;32m    614\u001b[0m     tools\u001b[38;5;241m.\u001b[39mappend(tool)\n\u001b[0;32m    615\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\langchain\\agents\\load_tools.py:301\u001b[0m, in \u001b[0;36m_get_serpapi\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_serpapi\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseTool:\n\u001b[0;32m    298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Tool(\n\u001b[0;32m    299\u001b[0m         name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSearch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    300\u001b[0m         description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA search engine. Useful for when you need to answer questions about current events. Input should be a search query.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m--> 301\u001b[0m         func\u001b[38;5;241m=\u001b[39mSerpAPIWrapper(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\u001b[38;5;241m.\u001b[39mrun,\n\u001b[0;32m    302\u001b[0m         coroutine\u001b[38;5;241m=\u001b[39mSerpAPIWrapper(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\u001b[38;5;241m.\u001b[39marun,\n\u001b[0;32m    303\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pydantic\\v1\\main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[0;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[1;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for SerpAPIWrapper\n__root__\n  Could not import serpapi python package. Please install it with `pip install google-search-results`. (type=value_error)"
     ]
    }
   ],
   "source": [
    "from langchain.agents import AgentType, initialize_agent, load_tools\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "# The tools we'll give the Agent access to. Note that the 'llm-math' tool uses an LLM, so we need to pass that in.\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
    "\n",
    "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "\n",
    "# Let's test it out!\n",
    "agent.run(\"What was the GDP of US in 2022 plus 5?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1c03fb26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-search-results in c:\\users\\nikit\\anaconda3\\lib\\site-packages (2.4.2)\n",
      "Requirement already satisfied: requests in c:\\users\\nikit\\anaconda3\\lib\\site-packages (from google-search-results) (2.28.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\nikit\\anaconda3\\lib\\site-packages (from requests->google-search-results) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nikit\\anaconda3\\lib\\site-packages (from requests->google-search-results) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\nikit\\anaconda3\\lib\\site-packages (from requests->google-search-results) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nikit\\anaconda3\\lib\\site-packages (from requests->google-search-results) (2023.7.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install google-search-results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a09db3e",
   "metadata": {},
   "source": [
    "# Memory : Langchain has memory inside it but bydefault it does not remember the previous context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "be91bc8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\"El Sabor Exquisito\" (The Exquisite Flavor)\n"
     ]
    }
   ],
   "source": [
    "chain = LLMChain(llm=llm,prompt=prompt_template_name)\n",
    "name = chain.run(\"Mexican\")\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ffe0c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\"Spice Palace\"\n"
     ]
    }
   ],
   "source": [
    "name = chain.run(\"Indian\")\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eefa634a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(chain.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14de984e",
   "metadata": {},
   "source": [
    "# ConversationBufferMemory : we have to import conversationBuffermemory in Langchain to remember the previous context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c74076eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\"El Sabor Exquisito\" (The Exquisite Flavor)\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory    ##we have to import conversationBuffermemory in Langchain to remember the previous context\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template_name, memory=memory)\n",
    "name = chain.run(\"Mexican\")\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7f002eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\"Al-Fawarish\" (meaning \"The Treasures\" in Arabic)\n"
     ]
    }
   ],
   "source": [
    "name = chain.run(\"Arabic\")\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7750030e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Mexican\n",
      "AI: \n",
      "\n",
      "\"El Sabor Exquisito\" (The Exquisite Flavor)\n",
      "Human: Arabic\n",
      "AI: \n",
      "\n",
      "\"Al-Fawarish\" (meaning \"The Treasures\" in Arabic)\n"
     ]
    }
   ],
   "source": [
    "print(chain.memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04db86d",
   "metadata": {},
   "source": [
    "# ConversationChain: it endlessely remember all the previous topics or context>>>>so, that it required very high pay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "252a2a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "{history}\n",
      "Human: {input}\n",
      "AI:\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "\n",
    "convo = ConversationChain(llm=OpenAI(temperature=0.7))\n",
    "print(convo.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "379cfe04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The first cricket world cup was held in 1975 and was won by the West Indies team.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"Who won the first cricket world cup?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "68e5bbab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  5+5 is equal to 10.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"How much is 5+5?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e1942664",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The captain of the winning West Indies team in the first cricket world cup was Clive Lloyd.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"Who was the captain ofthe winning team?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "901662d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Who won the first cricket world cup?\n",
      "AI:  The first cricket world cup was held in 1975 and was won by the West Indies team.\n",
      "Human: How much is 5+5?\n",
      "AI:   5+5 is equal to 10.\n",
      "Human: Who was the captain ofthe winning team?\n",
      "AI:  The captain of the winning West Indies team in the first cricket world cup was Clive Lloyd.\n"
     ]
    }
   ],
   "source": [
    "print(convo.memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0e2f5e",
   "metadata": {},
   "source": [
    "# ConversationBufferWindowMemory: it is used so that we can restrict this to remember the previous context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "578a0b7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The first cricket world cup was hosted in 1975 by England, and the West Indies team emerged as the winners. They defeated Australia in the final match by 17 runs.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "\n",
    "convo = ConversationChain(\n",
    "    llm=OpenAI(temperature=0.7),\n",
    "    memory=memory\n",
    ")\n",
    "convo.run(\"Who won the first cricket world cup?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3f55bf1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  The mathematical expression 5+5 equals 10.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"How much is 5+5?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b959b37c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' I am not familiar with the teams you are referring to, so I cannot provide an accurate answer. Do you have any other questions?'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"Who was the captain of the winning team?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8c2205",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
